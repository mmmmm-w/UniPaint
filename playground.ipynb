{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "\n",
    "from unipaint.pipelines.pipeline_unipaint import AnimationPipeline\n",
    "from unipaint.models.unet import UNet3DConditionModel\n",
    "from unipaint.models.sparse_controlnet import SparseControlNetModel\n",
    "from unipaint.models.unipaint.brushnet import BrushNetModel\n",
    "\n",
    "from unipaint.utils.util import load_weights,save_videos_grid\n",
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "from unipaint.utils.mask import RectangularMaskGenerator\n",
    "\n",
    "data = \"Running4\"\n",
    "path = \"models/StableDiffusion/stable-diffusion-v1-5\"\n",
    "brushnet_path = \"models/BrushNet/random_mask_brushnet_ckpt\"\n",
    "device = \"cuda:0\"\n",
    "dtype = torch.float16\n",
    "\n",
    "use_motion_module = True\n",
    "use_adapter = True\n",
    "\n",
    "motion_module_path = \"models/Motion_Module/v3_sd15_mm.ckpt\" if use_motion_module else \"\"\n",
    "adapter_path = \"models/Motion_Module/v3_sd15_adapter.ckpt\" if use_adapter else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 3D unet's pretrained weights from models/StableDiffusion/stable-diffusion-v1-5 ...\n",
      "### missing keys: 520; \n",
      "### unexpected keys: 0;\n",
      "### Motion Module Parameters: 417.1376 M\n"
     ]
    }
   ],
   "source": [
    "#load base model\n",
    "tokenizer        = CLIPTokenizer.from_pretrained(path, subfolder=\"tokenizer\", torch_dtype=dtype)\n",
    "text_encoder     = CLIPTextModel.from_pretrained(path, subfolder=\"text_encoder\").to(device,dtype)\n",
    "vae              = AutoencoderKL.from_pretrained(path, subfolder=\"vae\").to(device, dtype)\n",
    "\n",
    "inference_config = OmegaConf.load(\"configs/inference/inference-v3.yaml\")\n",
    "unet             = UNet3DConditionModel.from_pretrained_2d(path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs)).to(device, dtype)\n",
    "\n",
    "#load controlnet\n",
    "unet.config.num_attention_heads = 8\n",
    "unet.config.projection_class_embeddings_input_dim = None\n",
    "controlnet_config = OmegaConf.load(\"configs/inference/sparsectrl/latent_condition.yaml\")\n",
    "controlnet = SparseControlNetModel.from_unet(unet, controlnet_additional_kwargs=controlnet_config.get(\"controlnet_additional_kwargs\", {}))\n",
    "controlnet_state_dict = torch.load(\"models/Motion_Module/v3_sd15_sparsectrl_rgb.ckpt\", map_location=\"cpu\")\n",
    "controlnet_state_dict = controlnet_state_dict[\"controlnet\"] if \"controlnet\" in controlnet_state_dict else controlnet_state_dict\n",
    "controlnet_state_dict = {name: param for name, param in controlnet_state_dict.items() if \"pos_encoder.pe\" not in name}\n",
    "controlnet_state_dict.pop(\"animatediff_config\", \"\")\n",
    "controlnet.load_state_dict(controlnet_state_dict)\n",
    "controlnet.to(device, dtype)\n",
    "\n",
    "#load brushnet\n",
    "brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=dtype).to(device)\n",
    "\n",
    "#build pipeline\n",
    "pipeline = AnimationPipeline(\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
    "            controlnet=controlnet, brushnet = brushnet,\n",
    "            scheduler=DDIMScheduler(beta_start=0.00085,\n",
    "                                                beta_end=0.012,\n",
    "                                                beta_schedule=\"linear\",\n",
    "                                                steps_offset=0,\n",
    "                                                clip_sample=False)\n",
    "                                                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load motion module from models/Motion_Module/v3_sd15_mm.ckpt\n",
      "load domain lora from models/Motion_Module/v3_sd15_adapter.ckpt\n"
     ]
    }
   ],
   "source": [
    "pipeline = load_weights(\n",
    "    pipeline,\n",
    "    # motion module\n",
    "    motion_module_path         = motion_module_path,\n",
    "    motion_module_lora_configs = [],\n",
    "    # domain adapter\n",
    "    adapter_lora_path          = adapter_path,\n",
    "    adapter_lora_scale         = 1.0,\n",
    "    # image layers\n",
    "    dreambooth_model_path      = \"\",\n",
    "    lora_model_path            = \"\",\n",
    "    lora_alpha                 = 0.8,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Video\n",
    "Here we read frames of a video and generate a corresponding mask. The data is normalized to [0, 1], in shape (b f) c h w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = f\"outpaint_videos/SB_{data}.mp4\"\n",
    "vr = decord.VideoReader(video_path, width=512, height=512)\n",
    "\n",
    "video = vr.get_batch(list(range(0,16)))\n",
    "video = rearrange(video, \"f h w c -> c f h w\")\n",
    "frame = torch.clone(torch.unsqueeze(video/255, dim=0)).to(device, brushnet.dtype)\n",
    "mask_generator = RectangularMaskGenerator(mask_l=[0,0.4],\n",
    "                                          mask_r=[0,0.4],\n",
    "                                          mask_t=[0,0.4],\n",
    "                                          mask_b=[0,0.4])\n",
    "mask = mask_generator(frame)\n",
    "frame[mask==1]=0\n",
    "mask = mask.to(device, brushnet.dtype)\n",
    "frame = frame*2.-1.\n",
    "mask = mask*2.-1.\n",
    "\n",
    "save_videos_grid(((frame+1)/2).cpu(), f\"samples/{data}/masked_video.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.28it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 26.21it/s]\n",
      "100%|██████████| 25/25 [00:20<00:00,  1.25it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.40it/s]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"a man in blue, running\"\n",
    "n_prompt = \"worst quality, low quality, letterboxed\"\n",
    "sample = pipeline(\n",
    "    prompt = prompt,\n",
    "    negative_prompt     = n_prompt,\n",
    "    num_inference_steps = 25,\n",
    "    guidance_scale      = 12.5,\n",
    "    width               = 512,\n",
    "    height              = 512,\n",
    "    video_length        = 16,\n",
    "\n",
    "    controlnet_images = None,\n",
    "    controlnet_image_index = [0],\n",
    "    controlnet_conditioning_scale=0.0,\n",
    "\n",
    "    init_video = frame[:,:,:],\n",
    "    mask_video = mask[:,:,:],\n",
    "    brushnet_conditioning_scale = 1.0,\n",
    "    control_guidance_start = 0.0,\n",
    "    control_guidance_end = 1.0,\n",
    "    ).videos\n",
    "save_videos_grid(sample, f\"samples/{data}/brushnet_mm_{use_motion_module}_adapter_{use_adapter}.gif\")\n",
    "\n",
    "controlnet_images = torch.clone(frame)\n",
    "num_controlnet_images = controlnet_images.shape[2]\n",
    "controlnet_images = rearrange(controlnet_images, \"b c f h w -> (b f) c h w\")\n",
    "controlnet_images = vae.encode(controlnet_images).latent_dist.sample() * 0.18215\n",
    "controlnet_images = rearrange(controlnet_images, \"(b f) c h w -> b c f h w\", f=num_controlnet_images)\n",
    "\n",
    "sample = pipeline(\n",
    "    prompt = prompt,\n",
    "    negative_prompt     = n_prompt,\n",
    "    num_inference_steps = 25,\n",
    "    guidance_scale      = 12.5,\n",
    "    width               = 512,\n",
    "    height              = 512,\n",
    "    video_length        = 16,\n",
    "\n",
    "    controlnet_images = controlnet_images,\n",
    "    controlnet_image_index = [0],\n",
    "    controlnet_conditioning_scale=1.0\n",
    ").videos\n",
    "save_videos_grid(sample, f\"samples/{data}/controlnet_mm_{use_motion_module}_adapter_{use_adapter}.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trainable params: options\n",
    "    1. Motion Lora\n",
    "    2. Temporal layers in Brushnet (How?)\n",
    "    3. Whole motion module\n",
    "2. Data\n",
    "    1. WebVid or similar video datasets\n",
    "    2. Maybe some video segmentation dataset? These dataset should provide masks and corresponding tags\n",
    "3. How to train\n",
    "    - I have no experience of training a large model from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
