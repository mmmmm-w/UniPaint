{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "\n",
    "from unipaint.pipelines.pipeline_unipaint import AnimationPipeline\n",
    "from unipaint.models.unet import UNet3DConditionModel\n",
    "from unipaint.models.sparse_controlnet import SparseControlNetModel\n",
    "from unipaint.models.unipaint.brushnet import BrushNetModel\n",
    "\n",
    "from unipaint.utils.util import load_weights,save_videos_grid\n",
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "from unipaint.utils.mask import StaticRectangularMaskGenerator\n",
    "\n",
    "data = \"Running4\"\n",
    "path = \"models/StableDiffusion/stable-diffusion-v1-5\"\n",
    "brushnet_path = \"models/BrushNet/random_mask_brushnet_ckpt\"\n",
    "device = \"cuda:4\"\n",
    "dtype = torch.float16\n",
    "\n",
    "use_motion_module = True\n",
    "use_adapter = True\n",
    "\n",
    "motion_module_path = \"models/Motion_Module/v3_sd15_mm.ckpt\" if use_motion_module else \"\"\n",
    "adapter_path = \"models/Motion_Module/v3_sd15_adapter.ckpt\" if use_adapter else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 3D unet's pretrained weights from models/StableDiffusion/stable-diffusion-v1-5 ...\n",
      "### missing keys: 520; \n",
      "### unexpected keys: 0;\n",
      "### Motion Module Parameters: 417.1376 M\n"
     ]
    }
   ],
   "source": [
    "#load base model\n",
    "tokenizer        = CLIPTokenizer.from_pretrained(path, subfolder=\"tokenizer\", torch_dtype=dtype)\n",
    "text_encoder     = CLIPTextModel.from_pretrained(path, subfolder=\"text_encoder\").to(device,dtype)\n",
    "vae              = AutoencoderKL.from_pretrained(path, subfolder=\"vae\").to(device, dtype)\n",
    "\n",
    "inference_config = OmegaConf.load(\"configs/inference/inference-v3.yaml\")\n",
    "unet             = UNet3DConditionModel.from_pretrained_2d(path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs)).to(device, dtype)\n",
    "\n",
    "#load controlnet\n",
    "unet.config.num_attention_heads = 8\n",
    "unet.config.projection_class_embeddings_input_dim = None\n",
    "controlnet_config = OmegaConf.load(\"configs/inference/sparsectrl/latent_condition.yaml\")\n",
    "controlnet = SparseControlNetModel.from_unet(unet, controlnet_additional_kwargs=controlnet_config.get(\"controlnet_additional_kwargs\", {}))\n",
    "controlnet_state_dict = torch.load(\"models/Motion_Module/v3_sd15_sparsectrl_rgb.ckpt\", map_location=\"cpu\")\n",
    "controlnet_state_dict = controlnet_state_dict[\"controlnet\"] if \"controlnet\" in controlnet_state_dict else controlnet_state_dict\n",
    "controlnet_state_dict = {name: param for name, param in controlnet_state_dict.items() if \"pos_encoder.pe\" not in name}\n",
    "controlnet_state_dict.pop(\"animatediff_config\", \"\")\n",
    "controlnet.load_state_dict(controlnet_state_dict)\n",
    "controlnet.to(device, dtype)\n",
    "\n",
    "#load brushnet\n",
    "brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=dtype).to(device)\n",
    "\n",
    "#build pipeline\n",
    "pipeline = AnimationPipeline(\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
    "            controlnet=controlnet, brushnet = brushnet,\n",
    "            scheduler=DDIMScheduler(beta_start=0.00085,\n",
    "                                                beta_end=0.012,\n",
    "                                                beta_schedule=\"linear\",\n",
    "                                                steps_offset=0,\n",
    "                                                clip_sample=False)\n",
    "                                                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load motion module from models/Motion_Module/v3_sd15_mm.ckpt\n",
      "load domain lora from models/Motion_Module/v3_sd15_adapter.ckpt\n"
     ]
    }
   ],
   "source": [
    "pipeline = load_weights(\n",
    "    pipeline,\n",
    "    # motion module\n",
    "    motion_module_path         = motion_module_path,\n",
    "    motion_module_lora_configs = [],\n",
    "    # domain adapter\n",
    "    adapter_lora_path          = adapter_path,\n",
    "    adapter_lora_scale         = 1.0,\n",
    "    # image layers\n",
    "    dreambooth_model_path      = \"\",\n",
    "    lora_model_path            = \"\",\n",
    "    lora_alpha                 = 0.8,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Video\n",
    "Here we read frames of a video and generate a corresponding mask. The data is normalized to [0, 1], in shape (b f) c h w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = f\"outpaint_videos/SB_{data}.mp4\"\n",
    "vr = decord.VideoReader(video_path, width=512, height=512)\n",
    "\n",
    "video = vr.get_batch(list(range(0,16)))\n",
    "video = rearrange(video, \"f h w c -> c f h w\")\n",
    "frame = torch.clone(torch.unsqueeze(video/255, dim=0)).to(device, brushnet.dtype)\n",
    "del(vr)\n",
    "mask_generator = StaticRectangularMaskGenerator(mask_l=[0,0.4],\n",
    "                                          mask_r=[0,0.4],\n",
    "                                          mask_t=[0,0.4],\n",
    "                                          mask_b=[0,0.4])\n",
    "mask = mask_generator(frame)\n",
    "frame[mask==1]=0\n",
    "mask = mask.to(device, brushnet.dtype)\n",
    "frame = frame*2.-1.\n",
    "mask = mask*2.-1.\n",
    "\n",
    "save_videos_grid(((frame+1)/2).cpu(), f\"samples/{data}/masked_video.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:34<00:00,  1.38s/it]\n",
      "100%|██████████| 16/16 [00:01<00:00, 13.69it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 95.11 GiB of which 88.25 MiB is free. Process 381632 has 320.00 MiB memory in use. Process 381635 has 320.00 MiB memory in use. Process 381630 has 320.00 MiB memory in use. Process 381628 has 64.61 GiB memory in use. Process 381634 has 320.00 MiB memory in use. Process 381629 has 320.00 MiB memory in use. Process 381633 has 320.00 MiB memory in use. Process 381631 has 320.00 MiB memory in use. Process 3075899 has 28.17 GiB memory in use. Of the allocated memory 27.26 GiB is allocated by PyTorch, and 411.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m num_controlnet_images \u001b[38;5;241m=\u001b[39m controlnet_images\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     26\u001b[0m controlnet_images \u001b[38;5;241m=\u001b[39m rearrange(controlnet_images, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb c f h w -> (b f) c h w\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m controlnet_images \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrolnet_images\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlatent_dist\u001b[38;5;241m.\u001b[39msample() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.18215\u001b[39m\n\u001b[1;32m     28\u001b[0m controlnet_images \u001b[38;5;241m=\u001b[39m rearrange(controlnet_images, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(b f) c h w -> b c f h w\u001b[39m\u001b[38;5;124m\"\u001b[39m, f\u001b[38;5;241m=\u001b[39mnum_controlnet_images)\n\u001b[1;32m     30\u001b[0m sample \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     31\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m prompt,\n\u001b[1;32m     32\u001b[0m     negative_prompt     \u001b[38;5;241m=\u001b[39m n_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     41\u001b[0m     controlnet_conditioning_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m\n\u001b[1;32m     42\u001b[0m )\u001b[38;5;241m.\u001b[39mvideos\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/diffusers/models/vae.py:566\u001b[0m, in \u001b[0;36mAutoencoderKL.encode\u001b[0;34m(self, x, return_dict)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mFloatTensor, return_dict: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AutoencoderKLOutput:\n\u001b[0;32m--> 566\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m     moments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv(h)\n\u001b[1;32m    568\u001b[0m     posterior \u001b[38;5;241m=\u001b[39m DiagonalGaussianDistribution(moments)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/diffusers/models/vae.py:134\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;66;03m# down\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m down_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks:\n\u001b[0;32m--> 134\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mdown_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# middle\u001b[39;00m\n\u001b[1;32m    137\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block(sample)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:920\u001b[0m, in \u001b[0;36mDownEncoderBlock2D.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    919\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnets:\n\u001b[0;32m--> 920\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m downsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamplers:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/diffusers/models/resnet.py:493\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_shortcut \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    491\u001b[0m     input_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_shortcut(input_tensor)\n\u001b[0;32m--> 493\u001b[0m output_tensor \u001b[38;5;241m=\u001b[39m (\u001b[43minput_tensor\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mhidden_states\u001b[49m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_scale_factor\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_tensor\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 256.00 MiB. GPU 0 has a total capacity of 95.11 GiB of which 88.25 MiB is free. Process 381632 has 320.00 MiB memory in use. Process 381635 has 320.00 MiB memory in use. Process 381630 has 320.00 MiB memory in use. Process 381628 has 64.61 GiB memory in use. Process 381634 has 320.00 MiB memory in use. Process 381629 has 320.00 MiB memory in use. Process 381633 has 320.00 MiB memory in use. Process 381631 has 320.00 MiB memory in use. Process 3075899 has 28.17 GiB memory in use. Of the allocated memory 27.26 GiB is allocated by PyTorch, and 411.43 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "prompt = \"a man in blue, running\"\n",
    "n_prompt = \"worst quality, low quality, letterboxed\"\n",
    "sample = pipeline(\n",
    "    prompt = prompt,\n",
    "    negative_prompt     = n_prompt,\n",
    "    num_inference_steps = 25,\n",
    "    guidance_scale      = 12.5,\n",
    "    width               = 512,\n",
    "    height              = 512,\n",
    "    video_length        = 16,\n",
    "\n",
    "    controlnet_images = None,\n",
    "    controlnet_image_index = [0],\n",
    "    controlnet_conditioning_scale=0.0,\n",
    "\n",
    "    init_video = frame[:,:,:],\n",
    "    mask_video = mask[:,:,:],\n",
    "    brushnet_conditioning_scale = 1.0,\n",
    "    control_guidance_start = 0.0,\n",
    "    control_guidance_end = 1.0,\n",
    "    ).videos\n",
    "save_videos_grid(sample, f\"samples/{data}/brushnet_mm_{use_motion_module}_adapter_{use_adapter}.gif\")\n",
    "\n",
    "controlnet_images = torch.clone(frame)\n",
    "num_controlnet_images = controlnet_images.shape[2]\n",
    "controlnet_images = rearrange(controlnet_images, \"b c f h w -> (b f) c h w\")\n",
    "controlnet_images = vae.encode(controlnet_images).latent_dist.sample() * 0.18215\n",
    "controlnet_images = rearrange(controlnet_images, \"(b f) c h w -> b c f h w\", f=num_controlnet_images)\n",
    "\n",
    "sample = pipeline(\n",
    "    prompt = prompt,\n",
    "    negative_prompt     = n_prompt,\n",
    "    num_inference_steps = 25,\n",
    "    guidance_scale      = 12.5,\n",
    "    width               = 512,\n",
    "    height              = 512,\n",
    "    video_length        = 16,\n",
    "\n",
    "    controlnet_images = controlnet_images,\n",
    "    controlnet_image_index = [0],\n",
    "    controlnet_conditioning_scale=1.0\n",
    ").videos\n",
    "save_videos_grid(sample, f\"samples/{data}/controlnet_mm_{use_motion_module}_adapter_{use_adapter}.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible trainings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Trainable params: options\n",
    "    1. Motion Lora\n",
    "    2. Temporal layers in Brushnet (How?)\n",
    "    3. Whole motion module\n",
    "2. Data\n",
    "    1. WebVid or similar video datasets\n",
    "    2. Maybe some video segmentation dataset? These dataset should provide masks and corresponding tags\n",
    "3. How to train\n",
    "    - I have no experience of training a large model from scratch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 3D unet's pretrained weights from models/StableDiffusion/stable-diffusion-v1-5 ...\n",
      "### missing keys: 520; \n",
      "### unexpected keys: 0;\n",
      "### Motion Module Parameters: 417.1376 M\n",
      "load motion module from models/Motion_Module/v3_sd15_mm.ckpt\n",
      "load domain lora from models/Motion_Module/v3_sd15_adapter.ckpt\n",
      "global_step: 13000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "\n",
    "from unipaint.pipelines.pipeline_unipaint import AnimationPipeline\n",
    "from unipaint.models.unet import UNet3DConditionModel\n",
    "from unipaint.models.sparse_controlnet import SparseControlNetModel\n",
    "from unipaint.models.unipaint.brushnet import BrushNetModel\n",
    "\n",
    "from unipaint.utils.util import load_weights,save_videos_grid\n",
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "from unipaint.utils.mask import StaticRectangularMaskGenerator\n",
    "path = \"models/StableDiffusion/stable-diffusion-v1-5\"\n",
    "brushnet_path = \"models/BrushNet/random_mask_brushnet_ckpt\"\n",
    "device = \"cuda:4\"\n",
    "dtype = torch.float16\n",
    "\n",
    "use_motion_module = True\n",
    "use_adapter = True\n",
    "\n",
    "motion_module_path = \"models/Motion_Module/v3_sd15_mm.ckpt\" if use_motion_module else \"\"\n",
    "adapter_path = \"models/Motion_Module/v3_sd15_adapter.ckpt\" if use_adapter else \"\"\n",
    "\n",
    "#load base model\n",
    "tokenizer        = CLIPTokenizer.from_pretrained(path, subfolder=\"tokenizer\", torch_dtype=dtype)\n",
    "text_encoder     = CLIPTextModel.from_pretrained(path, subfolder=\"text_encoder\").to(device,dtype)\n",
    "vae              = AutoencoderKL.from_pretrained(path, subfolder=\"vae\").to(device, dtype)\n",
    "\n",
    "inference_config = OmegaConf.load(\"configs/inference/inference-v3.yaml\")\n",
    "unet             = UNet3DConditionModel.from_pretrained_2d(path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs)).to(device, dtype)\n",
    "\n",
    "#load brushnet\n",
    "brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=dtype).to(device)\n",
    "\n",
    "#build pipeline\n",
    "pipeline = AnimationPipeline(\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
    "            brushnet = brushnet,\n",
    "            scheduler=DDIMScheduler(beta_start=0.00085,\n",
    "                                                beta_end=0.012,\n",
    "                                                beta_schedule=\"linear\",\n",
    "                                                steps_offset=0,\n",
    "                                                clip_sample=False)\n",
    "                                                ).to(device)\n",
    "\n",
    "pipeline = load_weights(\n",
    "    pipeline,\n",
    "    # motion module\n",
    "    motion_module_path         = motion_module_path,\n",
    "    motion_module_lora_configs = [],\n",
    "    # domain adapter\n",
    "    adapter_lora_path          = adapter_path,\n",
    "    adapter_lora_scale         = 1.0,\n",
    "    # image layers\n",
    "    dreambooth_model_path      = \"\",\n",
    "    lora_model_path            = \"\",\n",
    "    lora_alpha                 = 0.8,\n",
    ").to(device)\n",
    "\n",
    "unet_checkpoint_path = \"outputs/unipaint_training_mask-2024-09-03T18-40-37/checkpoints/checkpoint.ckpt\"\n",
    "unet_checkpoint_path = torch.load(unet_checkpoint_path, map_location=\"cpu\")\n",
    "if \"global_step\" in unet_checkpoint_path: print(f\"global_step: {unet_checkpoint_path['global_step']}\")\n",
    "state_dict = unet_checkpoint_path[\"state_dict\"] if \"state_dict\" in unet_checkpoint_path else unet_checkpoint_path\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_key = k.replace('module.', '')  # Remove 'module.' from each key\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "m, u = unet.load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.34it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.36it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.35it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.35it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.32it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.35it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.34it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.34it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.36it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.28it/s]\n",
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.37it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "name = \"chicken\"\n",
    "if name == \"eagle\":\n",
    "    video_path = f\"outpaint_videos/SB_Eagle.mp4\"\n",
    "    prompt = \"a white head bald eagle\"\n",
    "    n_prompt = \"\"\n",
    "    data = np.load('SB_Eagle_mask.npz')\n",
    "if name == \"chicken\":\n",
    "    video_path = f\"outpaint_videos/SB_Eagle.mp4\"\n",
    "    prompt = \"a chicke\"\n",
    "    n_prompt = \"\"\n",
    "    data = np.load('SB_Eagle_mask.npz')\n",
    "if name == \"dog\":\n",
    "    video_path = f\"outpaint_videos/SB_Dog1.mp4\"\n",
    "    prompt = \"a white fluffy dog walking\"\n",
    "    n_prompt = \"\"\n",
    "    data = np.load('SB_Dog1_mask.npz')\n",
    "\n",
    "vr = decord.VideoReader(video_path, width=512, height=512)\n",
    "\n",
    "video = vr.get_batch(list(range(0,16)))\n",
    "video = rearrange(video, \"f h w c -> c f h w\")\n",
    "frame = torch.clone(torch.unsqueeze(video/255, dim=0)).to(device, brushnet.dtype)\n",
    "del(vr)\n",
    "\n",
    "mask = torch.tensor(np.unpackbits(data['mask']).reshape((16, 512, 512)))\n",
    "mask = torch.cat([mask.unsqueeze(dim=0)]*3,dim=0).unsqueeze(dim=0)\n",
    "frame[mask==1]=0\n",
    "mask = mask.to(device, brushnet.dtype)\n",
    "frame = frame*2.-1.\n",
    "mask = mask*2.-1.\n",
    "save_videos_grid(((frame+1)/2).cpu(), f\"./test_masked_video_{name}.gif\")\n",
    "samples = []\n",
    "samples.append(((frame+1)/2).cpu())\n",
    "for scale in np.arange(0.0, 1.1, 0.1).tolist():\n",
    "    sample = pipeline(\n",
    "        prompt = prompt,\n",
    "        negative_prompt     = n_prompt,\n",
    "        num_inference_steps = 25,\n",
    "        guidance_scale      = 12.5,\n",
    "        width               = 512,\n",
    "        height              = 512,\n",
    "        video_length        = 16,\n",
    "\n",
    "        init_video = frame[:,:,:],\n",
    "        mask_video = mask[:,:,:],\n",
    "        brushnet_conditioning_scale = scale,\n",
    "        control_guidance_start = 0.0,\n",
    "        control_guidance_end = 1.0,\n",
    "        ).videos\n",
    "    samples.append(sample)\n",
    "samples = torch.concat(samples)\n",
    "save_videos_grid(samples, f\"./sam_test_{name}.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unipaint.utils.mask import StaticRectangularMaskGenerator, MovingRectangularMaskGenerator, MarginalMaskGenerator, InterpolationMaskGenerator\n",
    "from omegaconf import OmegaConf\n",
    "config = OmegaConf.load(\"configs/training/v1/unipaint_training_mask.yaml\")\n",
    "mask_config = config.mask_config\n",
    "mask_generators = [\n",
    "        (StaticRectangularMaskGenerator(\n",
    "            mask_l=mask_config.static.mask_l,\n",
    "            mask_r=mask_config.static.mask_r,\n",
    "            mask_t=mask_config.static.mask_t,\n",
    "            mask_b=mask_config.static.mask_b\n",
    "        ), mask_config.static.mix_rate),\n",
    "        \n",
    "        (MovingRectangularMaskGenerator(\n",
    "            rect_height_range=mask_config.moving.rect_height_range,\n",
    "            rect_width_range=mask_config.moving.rect_width_range\n",
    "        ), mask_config.moving.mix_rate),\n",
    "        \n",
    "        (MarginalMaskGenerator(\n",
    "            mask_l=mask_config.marginal.mask_l,\n",
    "            mask_r=mask_config.marginal.mask_r,\n",
    "            mask_t=mask_config.marginal.mask_t,\n",
    "            mask_b=mask_config.marginal.mask_b\n",
    "        ), mask_config.marginal.mix_rate),\n",
    "        \n",
    "        (InterpolationMaskGenerator(\n",
    "            stride_range=mask_config.interpolation.stride_range\n",
    "        ), mask_config.interpolation.mix_rate)\n",
    "    ]\n",
    "\n",
    "# Define a placeholder for dataset-provided mask with its mix rate\n",
    "dataset_mask_mix_rate = mask_config.dataset_mask.mix_rate\n",
    "\n",
    "generators = [gen[0] for gen in mask_generators]\n",
    "\n",
    "mix_rates = [gen[1] for gen in mask_generators]\n",
    "mix_rates.append(dataset_mask_mix_rate)\n",
    "total_rate = sum(mix_rates)\n",
    "normalized_rates = [rate / total_rate for rate in mix_rates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unipaint.utils.mask.MarginalMaskGenerator object at 0x7f16f0dd37c0>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "temporary_generators = generators + [\"dataset_mask\"]\n",
    "pixel_values = torch.randn([2,16,3,256,256])\n",
    "toy_mask = torch.zeros_like(pixel_values)\n",
    "\n",
    "# Randomly select a mask generator or dataset mask based on mix rates\n",
    "selected_mask_generator = random.choices(temporary_generators, weights=normalized_rates, k=1)[0]\n",
    "print(selected_mask_generator)\n",
    "\n",
    "# Apply the selected mask to the data\n",
    "if selected_mask_generator == \"dataset_mask\":\n",
    "    mask = toy_mask  # Use the dataset-provided mask\n",
    "else:\n",
    "    mask = selected_mask_generator(rearrange(pixel_values, \"b f c h w -> b c f h w\"))  # Generate the mask using the selected generator\n",
    "    mask = rearrange(mask, \"b c f h w -> b f c h w\")\n",
    "\n",
    "masked_pixels = torch.clone(pixel_values)\n",
    "masked_pixels[mask==1] = -1 #Blackout the masked area\n",
    "mask = mask * 2 - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7616)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
