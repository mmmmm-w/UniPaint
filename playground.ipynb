{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 3D unet's pretrained weights from models/StableDiffusion/stable-diffusion-v1-5 ...\n",
      "### missing keys: 520; \n",
      "### unexpected keys: 0;\n",
      "### Motion Module Parameters: 417.1376 M\n",
      "load motion module from models/Motion_Module/v3_sd15_mm.ckpt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "\n",
    "from animatediff.pipelines.pipeline_unipaint import AnimationPipeline\n",
    "from animatediff.models.unet import UNet3DConditionModel\n",
    "from animatediff.models.sparse_controlnet import SparseControlNetModel\n",
    "\n",
    "from animatediff.utils.util import load_weights,save_videos_grid\n",
    "\n",
    "path = \"models/StableDiffusion/stable-diffusion-v1-5\"\n",
    "device = \"cuda:0\"\n",
    "dtype = torch.float16\n",
    "\n",
    "#load base model\n",
    "tokenizer        = CLIPTokenizer.from_pretrained(path, subfolder=\"tokenizer\", torch_dtype=dtype)\n",
    "text_encoder     = CLIPTextModel.from_pretrained(path, subfolder=\"text_encoder\").to(device,dtype)\n",
    "vae              = AutoencoderKL.from_pretrained(path, subfolder=\"vae\").to(device, dtype)\n",
    "\n",
    "inference_config = OmegaConf.load(\"configs/inference/inference-v3.yaml\")\n",
    "unet             = UNet3DConditionModel.from_pretrained_2d(path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs)).to(device, dtype)\n",
    "\n",
    "#load controlnet\n",
    "unet.config.num_attention_heads = 8\n",
    "unet.config.projection_class_embeddings_input_dim = None\n",
    "controlnet_config = OmegaConf.load(\"configs/inference/sparsectrl/latent_condition.yaml\")\n",
    "controlnet = SparseControlNetModel.from_unet(unet, controlnet_additional_kwargs=controlnet_config.get(\"controlnet_additional_kwargs\", {}))\n",
    "controlnet_state_dict = torch.load(\"models/Motion_Module/v3_sd15_sparsectrl_rgb.ckpt\", map_location=\"cpu\")\n",
    "controlnet_state_dict = controlnet_state_dict[\"controlnet\"] if \"controlnet\" in controlnet_state_dict else controlnet_state_dict\n",
    "controlnet_state_dict = {name: param for name, param in controlnet_state_dict.items() if \"pos_encoder.pe\" not in name}\n",
    "controlnet_state_dict.pop(\"animatediff_config\", \"\")\n",
    "controlnet.load_state_dict(controlnet_state_dict)\n",
    "controlnet.to(device, dtype)\n",
    "\n",
    "#build pipeline\n",
    "pipeline = AnimationPipeline(\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
    "            controlnet=controlnet,\n",
    "            scheduler=DDIMScheduler(beta_start=0.00085,\n",
    "                                                beta_end=0.012,\n",
    "                                                beta_schedule=\"linear\",\n",
    "                                                steps_offset=0,\n",
    "                                                clip_sample=False)\n",
    "                                                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load motion module from models/Motion_Module/v3_sd15_mm.ckpt\n",
      "load domain lora from models/Motion_Module/v3_sd15_adapter.ckpt\n"
     ]
    }
   ],
   "source": [
    "pipeline = load_weights(\n",
    "    pipeline,\n",
    "    # motion module\n",
    "    motion_module_path         = \"models/Motion_Module/v3_sd15_mm.ckpt\",\n",
    "    # motion_module_path         = \"\",\n",
    "    motion_module_lora_configs = [],\n",
    "    # domain adapter\n",
    "    adapter_lora_path          = \"models/Motion_Module/v3_sd15_adapter.ckpt\",\n",
    "    # adapter_lora_path          = \"\",\n",
    "    adapter_lora_scale         = 1.0,\n",
    "    # image layers\n",
    "    dreambooth_model_path      = \"\",\n",
    "    lora_model_path            = \"\",\n",
    "    lora_alpha                 = 0.8,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = transforms.ToTensor()\n",
    "controlnet_images = [image_transform(Image.open(\"__assets__/demos/image/interpolation_1.png\").convert(\"RGB\"))]\n",
    "controlnet_images = torch.stack(controlnet_images).unsqueeze(0).to(device)\n",
    "controlnet_images = rearrange(controlnet_images, \"b f c h w -> b c f h w\")\n",
    "num_controlnet_images = controlnet_images.shape[2]\n",
    "controlnet_images = rearrange(controlnet_images, \"b c f h w -> (b f) c h w\").to(dtype)\n",
    "controlnet_images = vae.encode(controlnet_images * 2. - 1.).latent_dist.sample() * 0.18215\n",
    "controlnet_images = rearrange(controlnet_images, \"(b f) c h w -> b c f h w\", f=num_controlnet_images)\n",
    "\n",
    "prompt = \"aerial view, beautiful forest, autumn, 4k, high quality\"\n",
    "n_prompt = \"worst quality, low quality, letterboxed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/25 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:16<00:00,  1.49it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 45.68it/s]\n"
     ]
    }
   ],
   "source": [
    "sample = pipeline(\n",
    "    prompt,\n",
    "    negative_prompt     = n_prompt,\n",
    "    num_inference_steps = 25,\n",
    "    guidance_scale      = 8.5,\n",
    "    width               = 384,\n",
    "    height              = 256,\n",
    "    video_length        = 16,\n",
    "\n",
    "    controlnet_images = controlnet_images,\n",
    "    controlnet_image_index = [0],\n",
    "    controlnet_conditioning_scale=1.0\n",
    ").videos\n",
    "save_videos_grid(sample, \"samples/baseline_test_with_control_no_adapter.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BrushNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from animatediff.models.unipaint.brushnet import BrushNetModel\n",
    "import torch\n",
    "\n",
    "brushnet_path = \"models/BrushNet/random_mask_brushnet_ckpt\"\n",
    "brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=dtype).to(device)\n",
    "pipeline.brushnet = brushnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Video\n",
    "Here we read frames of a video and generate a corresponding mask. The data is normalized to [0, 1], in shape (b f) c h w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "from scripts.mask import RectangularMaskGenerator\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "vr = decord.VideoReader(\"outpaint_videos/SB_Bear.mp4\", width=512, height=512)\n",
    "\n",
    "video = vr.get_batch(list(range(0,16)))\n",
    "video = rearrange(video, \"f h w c -> c f h w\")\n",
    "frame = torch.unsqueeze(video, dim=0)\n",
    "mask_generator = RectangularMaskGenerator(mask_l=[0,0.4],\n",
    "                                          mask_r=[0,0.4],\n",
    "                                          mask_t=[0,0.4],\n",
    "                                          mask_b=[0,0.4])\n",
    "mask = mask_generator(frame)\n",
    "frame[mask==1]=0\n",
    "frame = frame/255\n",
    "frame = frame.to(device, brushnet.dtype)\n",
    "mask = mask.to(device, brushnet.dtype)\n",
    "frame = frame*2-1\n",
    "mask = mask*2-1\n",
    "# original_mask=(mask.sum(1)[:,None,:,:] < 0).to(frame.dtype)\n",
    "# conditioning_latents=pipeline.vae.encode(frame).latent_dist.sample() * 0.18215\n",
    "# height, width = frame.shape[-2:]\n",
    "# mask = torch.nn.functional.interpolate(\n",
    "#             original_mask, \n",
    "#             size=(\n",
    "#                 conditioning_latents.shape[-2], \n",
    "#                 conditioning_latents.shape[-1]\n",
    "#             )\n",
    "#         )\n",
    "# conditioning_latents = torch.concat([conditioning_latents,mask],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:19<00:00,  1.27it/s]\n",
      "100%|██████████| 16/16 [00:00<00:00, 27.38it/s]\n"
     ]
    }
   ],
   "source": [
    "sample = pipeline(\n",
    "    prompt = \"bear walking in the forest\",\n",
    "    negative_prompt     = \"\",\n",
    "    num_inference_steps = 25,\n",
    "    guidance_scale      = 12.5,\n",
    "    width               = 512,\n",
    "    height              = 512,\n",
    "    video_length        = 16,\n",
    "\n",
    "    controlnet_images = None,\n",
    "    controlnet_image_index = [0],\n",
    "    controlnet_conditioning_scale=0.0,\n",
    "\n",
    "    init_video = frame[:,:,:],\n",
    "    mask_video = mask[:,:,:],\n",
    "    brushnet_conditioning_scale = 1.0,\n",
    "    control_guidance_start = 0.0,\n",
    "    control_guidance_end = 1.0,\n",
    "    ).videos\n",
    "save_videos_grid(sample, \"samples/baseline_test_Bear_with_brushnet.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_videos_grid(((frame+1)/2).cpu(), \"samples/masked_video_bear.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
