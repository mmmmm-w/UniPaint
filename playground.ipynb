{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDIMScheduler\n",
    "\n",
    "from unipaint.pipelines.pipeline_unipaint import AnimationPipeline\n",
    "from unipaint.models.unet import UNet3DConditionModel\n",
    "from unipaint.models.sparse_controlnet import SparseControlNetModel\n",
    "from unipaint.models.unipaint.brushnet3d import BrushNetModel\n",
    "\n",
    "from unipaint.utils.util import load_weights,save_videos_grid\n",
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "from unipaint.utils.mask import StaticRectangularMaskGenerator\n",
    "\n",
    "data = \"Running4\"\n",
    "path = \"models/StableDiffusion/stable-diffusion-v1-5\"\n",
    "brushnet_path = \"models/BrushNet/random_mask_brushnet_ckpt\"\n",
    "device = \"cuda:4\"\n",
    "dtype = torch.float16\n",
    "\n",
    "use_motion_module = True\n",
    "use_adapter = True\n",
    "\n",
    "motion_module_path = \"models/Motion_Module/v3_sd15_mm.ckpt\" if use_motion_module else \"\"\n",
    "adapter_path = \"models/Motion_Module/v3_sd15_adapter.ckpt\" if use_adapter else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 3D unet's pretrained weights from models/StableDiffusion/stable-diffusion-v1-5 ...\n",
      "### missing keys: 520; \n",
      "### unexpected keys: 0;\n",
      "### Motion Module Parameters: 417.1376 M\n"
     ]
    }
   ],
   "source": [
    "#load base model\n",
    "tokenizer        = CLIPTokenizer.from_pretrained(path, subfolder=\"tokenizer\", torch_dtype=dtype)\n",
    "text_encoder     = CLIPTextModel.from_pretrained(path, subfolder=\"text_encoder\").to(device,dtype)\n",
    "vae              = AutoencoderKL.from_pretrained(path, subfolder=\"vae\").to(device, dtype)\n",
    "\n",
    "inference_config = OmegaConf.load(\"configs/inference/inference-v3.yaml\")\n",
    "unet_additional_kwargs = OmegaConf.to_container(inference_config.unet_additional_kwargs)\n",
    "unet_additional_kwargs[\"unet_use_moe\"] = True\n",
    "unet             = UNet3DConditionModel.from_pretrained_2d(path, subfolder=\"unet\", unet_additional_kwargs=unet_additional_kwargs).to(device, dtype)\n",
    "\n",
    "#load controlnet\n",
    "unet.config.num_attention_heads = 8\n",
    "unet.config.projection_class_embeddings_input_dim = None\n",
    "controlnet_config = OmegaConf.load(\"configs/inference/sparsectrl/latent_condition.yaml\")\n",
    "controlnet = SparseControlNetModel.from_unet(unet, controlnet_additional_kwargs=controlnet_config.get(\"controlnet_additional_kwargs\", {}))\n",
    "controlnet_state_dict = torch.load(\"models/Motion_Module/v3_sd15_sparsectrl_rgb.ckpt\", map_location=\"cpu\")\n",
    "controlnet_state_dict = controlnet_state_dict[\"controlnet\"] if \"controlnet\" in controlnet_state_dict else controlnet_state_dict\n",
    "controlnet_state_dict = {name: param for name, param in controlnet_state_dict.items() if \"pos_encoder.pe\" not in name}\n",
    "controlnet_state_dict.pop(\"animatediff_config\", \"\")\n",
    "controlnet.load_state_dict(controlnet_state_dict)\n",
    "controlnet.to(device, dtype)\n",
    "\n",
    "#load brushnet\n",
    "brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=dtype).to(device)\n",
    "\n",
    "#build pipeline\n",
    "pipeline = AnimationPipeline(\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
    "            controlnet=controlnet, brushnet = brushnet,\n",
    "            scheduler=DDIMScheduler(beta_start=0.00085,\n",
    "                                    beta_end=0.012,\n",
    "                                    beta_schedule=\"linear\",\n",
    "                                    steps_offset=0,\n",
    "                                    clip_sample=False)\n",
    "            ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load motion module from models/Motion_Module/v3_sd15_mm.ckpt\n",
      "load domain lora from models/Motion_Module/v3_sd15_adapter.ckpt\n"
     ]
    }
   ],
   "source": [
    "pipeline = load_weights(\n",
    "    pipeline,\n",
    "    # motion module\n",
    "    motion_module_path         = motion_module_path,\n",
    "    motion_module_lora_configs = [],\n",
    "    # domain adapter\n",
    "    adapter_lora_path          = adapter_path,\n",
    "    adapter_lora_scale         = 1.0,\n",
    "    # image layers\n",
    "    dreambooth_model_path      = \"\",\n",
    "    lora_model_path            = \"\",\n",
    "    lora_alpha                 = 0.8,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Video\n",
    "Here we read frames of a video and generate a corresponding mask. The data is normalized to [0, 1], in shape (b f) c h w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = f\"outpaint_videos/SB_{data}.mp4\"\n",
    "vr = decord.VideoReader(video_path, width=512, height=512)\n",
    "\n",
    "video = vr.get_batch(list(range(0,16)))\n",
    "video = rearrange(video, \"f h w c -> c f h w\")\n",
    "frame = torch.clone(torch.unsqueeze(video/255, dim=0)).to(device, brushnet.dtype)\n",
    "del(vr)\n",
    "mask_generator = StaticRectangularMaskGenerator(mask_l=[0,0.4],\n",
    "                                          mask_r=[0,0.4],\n",
    "                                          mask_t=[0,0.4],\n",
    "                                          mask_b=[0,0.4])\n",
    "mask = mask_generator(frame)\n",
    "frame[mask==1]=0\n",
    "mask = mask.to(device, brushnet.dtype)\n",
    "frame = frame*2.-1.\n",
    "mask = mask*2.-1.\n",
    "\n",
    "save_videos_grid(((frame+1)/2).cpu(), f\"samples/{data}/masked_video.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a man in blue, running\"\n",
    "n_prompt = \"worst quality, low quality, letterboxed\"\n",
    "sample = pipeline(\n",
    "    prompt = prompt,\n",
    "    negative_prompt     = n_prompt,\n",
    "    num_inference_steps = 25,\n",
    "    guidance_scale      = 12.5,\n",
    "    width               = 512,\n",
    "    height              = 512,\n",
    "    video_length        = 16,\n",
    "\n",
    "    controlnet_images = None,\n",
    "    controlnet_image_index = [0],\n",
    "    controlnet_conditioning_scale=0.0,\n",
    "\n",
    "    init_video = frame[:,:,:],\n",
    "    mask_video = mask[:,:,:],\n",
    "    brushnet_conditioning_scale = 1.0,\n",
    "    control_guidance_start = 0.0,\n",
    "    control_guidance_end = 1.0,\n",
    "    ).videos\n",
    "save_videos_grid(sample, f\"samples/{data}/brushnet_mm_{use_motion_module}_adapter_{use_adapter}.gif\")\n",
    "\n",
    "controlnet_images = torch.clone(frame)\n",
    "num_controlnet_images = controlnet_images.shape[2]\n",
    "controlnet_images = rearrange(controlnet_images, \"b c f h w -> (b f) c h w\")\n",
    "controlnet_images = vae.encode(controlnet_images).latent_dist.sample() * 0.18215\n",
    "controlnet_images = rearrange(controlnet_images, \"(b f) c h w -> b c f h w\", f=num_controlnet_images)\n",
    "\n",
    "sample = pipeline(\n",
    "    prompt = prompt,\n",
    "    negative_prompt     = n_prompt,\n",
    "    num_inference_steps = 25,\n",
    "    guidance_scale      = 12.5,\n",
    "    width               = 512,\n",
    "    height              = 512,\n",
    "    video_length        = 16,\n",
    "\n",
    "    controlnet_images = controlnet_images,\n",
    "    controlnet_image_index = [0],\n",
    "    controlnet_conditioning_scale=1.0\n",
    ").videos\n",
    "save_videos_grid(sample, f\"samples/{data}/controlnet_mm_{use_motion_module}_adapter_{use_adapter}.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 3D unet's pretrained weights from models/StableDiffusion/stable-diffusion-v1-5 ...\n",
      "### missing keys: 520; \n",
      "### unexpected keys: 0;\n",
      "### Motion Module Parameters: 417.1376 M\n",
      "Marked down_blocks.0.motion_modules.0.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked down_blocks.0.motion_modules.1.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked down_blocks.1.motion_modules.0.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked down_blocks.1.motion_modules.1.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked down_blocks.2.motion_modules.0.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked down_blocks.2.motion_modules.1.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked down_blocks.3.motion_modules.0.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked down_blocks.3.motion_modules.1.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.0.motion_modules.0.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.0.motion_modules.1.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.0.motion_modules.2.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.1.motion_modules.0.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.1.motion_modules.1.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.1.motion_modules.2.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.2.motion_modules.0.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.2.motion_modules.1.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.2.motion_modules.2.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.3.motion_modules.0.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.3.motion_modules.1.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "Marked up_blocks.3.motion_modules.2.temporal_transformer.transformer_blocks.0.ff for replacement with MoEFFN\n",
      "global_step: 29620\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from einops import rearrange\n",
    "\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import AutoencoderKL, DDIMScheduler, DDPMScheduler\n",
    "\n",
    "from unipaint.pipelines.pipeline_unipaint import AnimationPipeline\n",
    "from unipaint.models.unet import UNet3DConditionModel\n",
    "from unipaint.models.sparse_controlnet import SparseControlNetModel\n",
    "from unipaint.models.unipaint.brushnet3d import BrushNetModel\n",
    "\n",
    "from unipaint.utils.util import load_weights,save_videos_grid\n",
    "from unipaint.utils.convert_to_moe import replace_ffn_with_moeffn,task_context\n",
    "import decord\n",
    "decord.bridge.set_bridge(\"torch\")\n",
    "from unipaint.utils.mask import StaticRectangularMaskGenerator\n",
    "path = \"models/StableDiffusion/stable-diffusion-v1-5\"\n",
    "brushnet_path = \"models/BrushNet/random_mask_brushnet_ckpt\"\n",
    "device = \"cuda:4\"\n",
    "dtype = torch.float16\n",
    "\n",
    "use_motion_module = False\n",
    "use_adapter = False\n",
    "\n",
    "motion_module_path = \"models/Motion_Module/v3_sd15_mm.ckpt\" if use_motion_module else \"\"\n",
    "adapter_path = \"models/Motion_Module/v3_sd15_adapter.ckpt\" if use_adapter else \"\"\n",
    "\n",
    "#load base model\n",
    "tokenizer        = CLIPTokenizer.from_pretrained(path, subfolder=\"tokenizer\", torch_dtype=dtype)\n",
    "text_encoder     = CLIPTextModel.from_pretrained(path, subfolder=\"text_encoder\").to(device,dtype)\n",
    "vae              = AutoencoderKL.from_pretrained(path, subfolder=\"vae\").to(device, dtype)\n",
    "\n",
    "inference_config = OmegaConf.load(\"configs/inference/inference-v3.yaml\")\n",
    "unet             = UNet3DConditionModel.from_pretrained_2d(path, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.unet_additional_kwargs)).to(device, dtype)\n",
    "\n",
    "#load brushnet\n",
    "brushnet = BrushNetModel.from_pretrained(brushnet_path, torch_dtype=dtype).to(device)\n",
    "\n",
    "unet = replace_ffn_with_moeffn(unet)\n",
    "unet_checkpoint_path = \"./checkpoints/mixed_4.ckpt\"\n",
    "unet_checkpoint_path = torch.load(unet_checkpoint_path, map_location=\"cpu\")\n",
    "if \"global_step\" in unet_checkpoint_path: print(f\"global_step: {unet_checkpoint_path['global_step']}\")\n",
    "state_dict = unet_checkpoint_path[\"state_dict\"] if \"state_dict\" in unet_checkpoint_path else unet_checkpoint_path\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_key = k.replace('module.', '')  # Remove 'module.' from each key\n",
    "    new_state_dict[new_key] = v\n",
    "\n",
    "m, u = unet.load_state_dict(new_state_dict, strict=False)\n",
    "\n",
    "#build pipeline\n",
    "pipeline = AnimationPipeline(\n",
    "            vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,\n",
    "            brushnet = brushnet,\n",
    "            scheduler=DDIMScheduler(beta_start=0.00085,\n",
    "                                    beta_end=0.012,\n",
    "                                    beta_schedule=\"linear\",\n",
    "                                    steps_offset=0,\n",
    "                                    clip_sample=False)\n",
    "                                ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m samples\u001b[38;5;241m.\u001b[39mappend(((frame\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu())\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m task_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutpaint\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 27\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnegative_prompt\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_inference_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[43mguidance_scale\u001b[49m\u001b[43m      \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m12.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m               \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m              \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_video\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_video\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbrushnet_conditioning_scale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontrol_guidance_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontrol_guidance_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvideos\n\u001b[1;32m     42\u001b[0m     samples\u001b[38;5;241m.\u001b[39mappend(sample)\n\u001b[1;32m     43\u001b[0m samples \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(samples)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/tuna/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/zwan/UniPaint/unipaint/pipelines/pipeline_unipaint.py:515\u001b[0m, in \u001b[0;36mAnimationPipeline.__call__\u001b[0;34m(self, prompt, video_length, height, width, num_inference_steps, guidance_scale, negative_prompt, num_videos_per_prompt, eta, generator, latents, output_type, return_dict, callback, callback_steps, controlnet_images, controlnet_image_index, controlnet_conditioning_scale, init_video, mask_video, brushnet_conditioning_scale, control_guidance_start, control_guidance_end, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m brushnet_prompt_embeds \u001b[38;5;241m=\u001b[39m text_embeddings\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(brushnet_keep[i], \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m--> 515\u001b[0m     cond_scale \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;241m*\u001b[39m s \u001b[38;5;28;01mfor\u001b[39;00m c, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbrushnet_conditioning_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbrushnet_keep\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    517\u001b[0m     brushnet_cond_scale \u001b[38;5;241m=\u001b[39m brushnet_conditioning_scale\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "vr = decord.VideoReader(\"outpaint_videos/CI/CI_Hall.mp4\", width=-1, height=512)\n",
    "prompt = \"The atmosphere is mysterious and ancient, with white greek columns. Extremely realistic, high resolution.\"\n",
    "n_prompt = \"worst quality, letter boxed, random shade, a mess\"\n",
    "\n",
    "video = vr.get_batch(list(range(0,16)))\n",
    "video = rearrange(video, \"f h w c -> c f h w\")\n",
    "frame = torch.clone(torch.unsqueeze(video/255, dim=0)).to(device, brushnet.dtype)\n",
    "del(vr)\n",
    "\n",
    "\n",
    "width = frame.shape[-1]\n",
    "margin = int((width-512)/2)\n",
    "frame = frame[...,margin:margin+512]\n",
    "mask = torch.zeros_like(frame)\n",
    "mask[...,:150,:] = 1\n",
    "mask[...,500:,:] = 1\n",
    "mask[...,:200] = 1\n",
    "mask[...,300:] = 1\n",
    "frame[mask==1]=0\n",
    "mask = mask.to(device, brushnet.dtype)\n",
    "frame = frame*2.-1.\n",
    "mask = mask*2.-1.\n",
    "samples = []\n",
    "samples.append(((frame+1)/2).cpu())\n",
    "with task_context(\"outpaint\"):\n",
    "    sample = pipeline(\n",
    "        prompt = prompt,\n",
    "        negative_prompt     = n_prompt,\n",
    "        num_inference_steps = 100,\n",
    "        guidance_scale      = 12.5,\n",
    "        width               = 512,\n",
    "        height              = 512,\n",
    "        video_length        = 16,\n",
    "\n",
    "        init_video = frame,\n",
    "        mask_video = mask,\n",
    "        brushnet_conditioning_scale = 1.0,\n",
    "        control_guidance_start = 0.0,\n",
    "        control_guidance_end = 1.0,\n",
    "        ).videos\n",
    "    samples.append(sample)\n",
    "samples = torch.concat(samples)\n",
    "save_videos_grid(samples, f\"./test.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "name = \"chicken\"\n",
    "if name == \"eagle\":\n",
    "    video_path = f\"outpaint_videos/SB_Eagle.mp4\"\n",
    "    prompt = \"a white head bald eagle\"\n",
    "    n_prompt = \"\"\n",
    "    data = np.load('SB_Eagle_mask.npz')\n",
    "if name == \"chicken\":\n",
    "    video_path = f\"outpaint_videos/SB_Eagle.mp4\"\n",
    "    prompt = \"a chicke\"\n",
    "    n_prompt = \"\"\n",
    "    data = np.load('samples/SB_Eagle_mask.npz')\n",
    "if name == \"dog\":\n",
    "    video_path = f\"outpaint_videos/SB_Dog1.mp4\"\n",
    "    prompt = \"a white fluffy dog walking\"\n",
    "    n_prompt = \"\"\n",
    "    data = np.load('SB_Dog1_mask.npz')\n",
    "\n",
    "vr = decord.VideoReader(video_path, width=512, height=512)\n",
    "\n",
    "video = vr.get_batch(list(range(0,16)))\n",
    "video = rearrange(video, \"f h w c -> c f h w\")\n",
    "frame = torch.clone(torch.unsqueeze(video/255, dim=0)).to(device, brushnet.dtype)\n",
    "del(vr)\n",
    "\n",
    "mask = torch.tensor(np.unpackbits(data['mask']).reshape((16, 512, 512)))\n",
    "mask = torch.cat([mask.unsqueeze(dim=0)]*3,dim=0).unsqueeze(dim=0)\n",
    "frame[mask==1]=0\n",
    "mask = mask.to(device, brushnet.dtype)\n",
    "frame = frame*2.-1.\n",
    "mask = mask*2.-1.\n",
    "save_videos_grid(((frame+1)/2).cpu(), f\"./test_masked_video_{name}.gif\")\n",
    "samples = []\n",
    "samples.append(((frame+1)/2).cpu())\n",
    "for scale in np.arange(0.0, 1.1, 0.1).tolist():\n",
    "    sample = pipeline(\n",
    "        prompt = prompt,\n",
    "        negative_prompt     = n_prompt,\n",
    "        num_inference_steps = 25,\n",
    "        guidance_scale      = 12.5,\n",
    "        width               = 512,\n",
    "        height              = 512,\n",
    "        video_length        = 16,\n",
    "\n",
    "        init_video = frame[:,:,:],\n",
    "        mask_video = mask[:,:,:],\n",
    "        brushnet_conditioning_scale = scale,\n",
    "        control_guidance_start = 0.0,\n",
    "        control_guidance_end = 1.0,\n",
    "        ).videos\n",
    "    samples.append(sample)\n",
    "samples = torch.concat(samples)\n",
    "save_videos_grid(samples, f\"./sam_test_{name}.gif\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tuna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
